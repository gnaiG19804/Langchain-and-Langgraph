{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db46cea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='2 + 2 = 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 43, 'total_tokens': 52, 'completion_time': 0.012032902, 'completion_tokens_details': None, 'prompt_time': 0.001168913, 'prompt_tokens_details': None, 'queue_time': 0.057121647, 'total_time': 0.013201815}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_dae98b5ecb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bfd5c-f6b5-72c2-a1d1-7c6842e36f1b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 43, 'output_tokens': 9, 'total_tokens': 52})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "# ===== ENV =====\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "result = llm.invoke(\"What is 2 + 2?\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0f3643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list= [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter   = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore     = FAISS.from_documents(doc_splits, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790fc724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "  \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "  binary_score: str = Field(\n",
    "    description=\"Documents are relevant to the question, 'yes' or 'no'.\"\n",
    "  )\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "structured_llm_grade = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"You are an expert assistant that helps users find relevant information from a set of documents to answer their questions. Your task is to evaluate the relevance of the provided documents in relation to the user's question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\",\"Retrived documents: {documents}\\n\\nUser question: {question}\")\n",
    "  ])\n",
    "\n",
    "retriever_grader= grade_prompt | structured_llm_grade\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retriever_grader.invoke({\"question\": question, \"documents\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81c24f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent memory refers to the component of a LLM-powered autonomous agent system that enables agents to behave conditioned on past experience. The memory mechanism is a key component of the agent system, complementing the LLM brain. The context mentions \"Types of Memory\" and \"Maximum Inner Product Search (MIPS)\" as related concepts, but does not provide further details on how agent memory works.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 1. Khởi tạo LLM\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "template_text = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template_text)\n",
    "\n",
    "# 3. Hàm xử lý format\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "formatted_context = format_docs(docs) \n",
    "generation = rag_chain.invoke({\n",
    "    \"context\": formatted_context, \n",
    "    \"question\": question\n",
    "})\n",
    "\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b7ef46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a more specific and improved question: \\n\\n\"What are the current advancements and techniques in artificial intelligence for improving agent memory in multi-agent systems?\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "system = \"\"\"You are an expert assistant that helps users find relevant information from a set of documents to answer their questions. Your task is to evaluate the relevance of the provided documents in relation to the user's question. If the documents are not relevant, respond with 'no'. If they are relevant, respond with 'yes'.\"\"\"\n",
    "\n",
    "re_writer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\",\"Here is the initial question: \\n\\n {question} \\n Fromulate an improved question.\"),\n",
    "  ])\n",
    "question_rewriter = re_writer_prompt | llm | StrOutputParser()\n",
    "\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
